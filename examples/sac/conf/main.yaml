defaults:
  - overrides: heating
  - _self_


env:
  name: ${overrides.env_name}
  task: ""
  exp_name: ${env.name}_SAC
  library: gym
  max_episode_steps: ${overrides.max_steps}
  seed: 42

# collector
collector:
  total_frames: 200_000
  init_random_frames: 1_000
  frames_per_batch: 1000
  init_env_steps: 1000
  device: cpu
  env_per_collector: 1
  reset_at_each_iter: False

# replay buffer
replay_buffer:
  size: 1000000
  prb: 0 # use prioritized experience replay
  scratch_dir: ${env.exp_name}_${env.seed}

# optim
optim:
  utd_ratio: 1.0
  gamma: 0.99
  loss_function: l2
  lr: 3.0e-4
  weight_decay: 0.0
  batch_size: 256
  target_update_polyak: 0.995
  alpha_init: 1.0
  adam_eps: 1.0e-8

# network
network:
  hidden_sizes: [256, 256]
  activation: relu
  default_policy_scale: 1.0
  scale_lb: 0.1
  device: cpu

# logging
logger: tensorboard
exp_name: default
env_name: ${overrides.env_name}
eval_iter: 5000

hydra:
  run:
    dir: ./outputs/sac/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: True

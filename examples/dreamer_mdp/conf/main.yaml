defaults:
  - overrides: cartpole_meta
  - _self_


# env cfg
env_name: ${overrides.env_name}
termination_fns: ${overrides.termination_fns}
reward_fns: ${overrides.reward_fns}
env_max_steps: ${overrides.env_max_steps}

# learning
meta_train_frames: ${overrides.meta_train_frames}
meta_train_init_frames: ${overrides.meta_train_init_frames}
meta_train_logits_frames: ${overrides.meta_train_logits_frames}
frames_per_batch: ${overrides.frames_per_batch}
normalize_rewards_online: ${overrides.normalize_rewards_online}

# policy
discount_loss: True
pred_continue: True
imagination_horizon: ${overrides.imagination_horizon}

# model learning
model_type: causal
mask_type: direct
alpha: 1.
sigmoid_threshold: 0.1
buffer_size: 10000
model_optim_steps_per_batch: ${overrides.model_optim_steps_per_batch}
policy_optim_steps_per_batch: ${overrides.policy_optim_steps_per_batch}
train_mask_iters: 10
train_model_iters: 40

batch_size: ${overrides.batch_size}
batch_length: ${overrides.batch_length}

#world_model_lr: 5e-4
#actor_lr: 3e-5
#critic_lr: 3e-5
world_model_lr: 3e-4
actor_lr: 3e-5
critic_lr: 3e-5

context_lr: 0.001
observed_logits_lr: 0.002
context_logits_lr: 0.002
world_model_weight_decay: 0.00001
world_model_grad_clip: 100.
actor_grad_clip: 100.
critic_grad_clip: 100.
hidden_size: 200
hidden_layers: 4
lambda_transition: 1.0
lambda_reward: 1.0
lambda_terminated: 1.0
lambda_mutual_info: 0.0
lambda_entropy:   0.0

sparse_weight: ${overrides.sparse_weight}
context_sparse_weight: ${overrides.context_sparse_weight}
context_max_weight: ${overrides.context_max_weight}
sampling_times: 30

# log
exp_name: default
logger: tensorboard
offline_logging: False
eval_interval: ${overrides.eval_interval}
eval_repeat_nums: 1
eval_record_nums: 0
save_model_interval: 10

# meta-RL
meta: ${overrides.meta}
max_context_dim: 10
task_num: 50
meta_test_task_num: 20
meta_test_interval: 100
meta_test_frames: ${overrides.meta_test_frames}
oracle_context: ${overrides.oracle_context}
new_oracle_context: ${overrides.new_oracle_context}

# other
model_device: cuda:0
collector_device: ${model_device}
seed: 42

hydra:
  run:
    dir: ./outputs/dreamer_mdp/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: True

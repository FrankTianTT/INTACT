defaults:
  - overrides: cartpole
  - _self_


# env cfg
env_name: ${overrides.env_name}
termination_fns: ${overrides.termination_fns}
reward_fns: ${overrides.reward_fns}
env_max_steps: ${overrides.env_max_steps}

# learning
train_frames_per_task: ${overrides.train_frames_per_task}
init_frames_per_task: ${overrides.init_frames_per_task}

# save
save_model_frames_per_task: 1000

# planning
planning_horizon: ${overrides.planning_horizon}
optim_steps: 5
num_candidates: 350
top_k: 35

# model learning
model_type: plain
reinforce: True
batch_size: 256
buffer_size: 10000
optim_steps_per_frame: 1
train_mask_iters: 10
train_model_iters: 40
world_model_lr: 0.001
context_lr: 0.001
observed_logits_lr: 0.002
context_logits_lr: 0.002
world_model_weight_decay: 0.001
hidden_size: 256
hidden_layers: 4

lambda_transition: 1.0
lambda_reward: 1.0
lambda_terminated: 10.0
lambda_mutual_info: 0.0

sparse_weight: ${overrides.sparse_weight}
context_sparse_weight: ${overrides.context_sparse_weight}
context_max_weight: ${overrides.context_max_weight}
sampling_times: 30

# log
exp_name: default
logger: tensorboard
offline_logging: False
eval_interval_frames_per_task: ${overrides.eval_interval_frames_per_task}
eval_repeat_nums: 3
eval_record_nums: 1

# envs-RL
meta: True
max_context_dim: 10
task_num: 50
meta_test_task_num: 20
meta_test_interval_frames_per_task: ${overrides.meta_test_interval_frames_per_task}
meta_task_adjust_frames_per_task: 50
meta_test_model_learning_per_frame: 5
oracle_context: ${overrides.oracle_context}
new_oracle_context: ${overrides.new_oracle_context}

# other
model_device: cuda:0
collector_device: cpu
seed: 42

hydra:
  run:
    dir: ./outputs/mpc/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: True

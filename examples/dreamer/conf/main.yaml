defaults:
  - overrides: cartpole
  - _self_

# env cfg
env_name: ${overrides.env_name}
env_max_steps: ${overrides.env_max_steps}


train_frames_per_task: ${overrides.train_frames_per_task}
frames_per_batch: 800
init_frames_per_task: ${overrides.init_frames_per_task}
policy_learning_frames_per_task: 0

batch_size: 50
batch_length: 25
train_agent_frames: 0

optim_steps_per_batch: 80
buffer_size: 25000

world_model_lr: 6e-4
actor_value_lr: 8e-5
context_lr: 0.001
observed_logits_lr: 0.001
context_logits_lr: 0.001

# logger
eval_interval: 4
eval_repeat_nums: 3
eval_record_nums: 0
exp_name: default
logger: wandb
#logger: tensorboard
offline_logging: False

# model learning
model_type: causal
reinforce: False
alpha: 1.
variable_num: 10
state_dim_per_variable: 3
belief_dim_per_variable: 20
disable_belief: False
using_cross_belief: False
free_nats: 3.0

sparse_weight: ${overrides.sparse_weight}
context_sparse_weight: ${overrides.context_sparse_weight}
context_max_weight: ${overrides.context_max_weight}
sampling_times: 30
residual: True
hidden_size: 400

# actor
actor_dist_type: tanh_normal

# meta
meta: False
meta_test_interval: ${overrides.meta_test_interval}
max_context_dim: 5
task_num: 50
meta_test_task_num: 20
meta_task_adjust_frames_per_task: 5000
oracle_context: ${overrides.oracle_context}
new_oracle_context: ${overrides.new_oracle_context}


lambda_kl: 1.0
lambda_reco: 1.0
lambda_reward: 10.0
lambda_continue: 10.0
imagination_horizon: 15
discount_loss: True
pred_continue: True

train_mask_iters: 10
#train_mask_iters: 0
train_model_iters: 40


grad_clip: 100

normalize_rewards_online: False



# other
model_device: cuda:0
collector_device: cpu
seed: 42

hydra:
  run:
    dir: ./outputs/dreamer/${now:%Y-%m-%d}/${now:%H-%M-%S}
  job:
    chdir: True

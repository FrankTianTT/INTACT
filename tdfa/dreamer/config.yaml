################################################################################
# Environment Config
################################################################################
env_library: "gym"
# env_library used for the simulated environment. Default=gym
env_name: "CartPoleContinuous-v0"
# name of the environment to be created. Default=Humanoid-v2
env_task: ""
# task (if any) for the environment. Default=run
categorical_action_encoding: False
# whether the action space is categorical or not. Only used in Gym Env. Default=False
from_pixels: True
# whether the environment output should be state vector(s) (default) or the pixels.
frame_skip: 1
# frame_skip for the environment. Note that this value does NOT impact the buffer size,
# maximum steps per trajectory, frames per batch or any other factor in the algorithm,
# e.g. if the total number of frames that has to be computed is 50e6 and the frame skip is 4
# the actual number of frames retrieved will be 200e6. Default=1.
reward_scaling: 1.0
# scale of the reward.
reward_loc: 0.0
# location of the reward.
init_env_steps: 1000
# number of random steps to compute normalizing constants
vecnorm: False
# Normalizes the environment observation and reward outputs with the running statistics obtained across processes.
norm_rewards: False
# If True, rewards will be normalized on the fly. This may interfere with SAC update rule and should be used cautiously.
norm_stats: True
# Deactivates the normalization based on random collection of data.
noops: 0
# number of random steps to do after reset. Default is 0
catframes: 1
# Number of frames to concatenate through time. Default is 0 (do not use CatFrames).
center_crop: False
# center crop size.
grayscale: False
# Disables grayscale transform.
max_frames_per_traj: 1000
# Number of steps before a reset of the environment is called (if it has not been flagged as done before).
batch_transform: True
# if True, the transforms will be applied to the parallel env, and not to each individual env.\
image_size: 64
# size of the image to be returned by the environment. Default=64


################################################################################
# Dreamer World Model Config
################################################################################
variable_num: 10
state_dim_per_variable: 3
hidden_dim_per_variable: 20
rnn_input_dim_per_variable: 20
residual: true
logits_clip: 3.0
scale_lb: 0.1
mlp_num_units: 400


# meta
max_context_dim: 0
task_num: 0

################################################################################
# Dreamer Algorithm Config
################################################################################
batch_length:  25
state_dim:  30
rssm_hidden_dim:  200
grad_clip:  100
world_model_lr: float = 6e-4
actor_value_lr: float = 8e-5
imagination_horizon:  15
# Decay of the reward moving averaging
exploration:  "additive_gaussian"
# One of "additive_gaussian", "ou_exploration" or ""


################################################################################
# Logger Config
################################################################################
logger: wandb
offline_logging: False
exp_name: default
record_video: 0



async_collection: True
batch_size: 50
total_frames: 5000000

# we want 50 frames / traj in the replay buffer. Given the frame_skip=2 this makes each traj 100 steps long
env_per_collector: 8
num_workers: 8
frames_per_batch: 800
optim_steps_per_batch: 80
record_interval: 30
record_frames: 1000
buffer_size: 20000

init_random_frames: 5000

normalize_rewards_online: True
normalize_rewards_online_scale: 5.0
normalize_rewards_online_decay: 0.99999


model_device: "cuda:0"
collector_device: "cuda:1"